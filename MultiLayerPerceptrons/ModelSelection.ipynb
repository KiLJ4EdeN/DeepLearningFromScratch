{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ModelSelection.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "EF21dfZPrsdb"
      },
      "source": [
        "!pip install mxnet gluon d2l"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nd3MV3vyrOzW"
      },
      "source": [
        "# generilization basically,\n",
        "# we want to create models that exctract and show useful information from the dataset.\n",
        "# typically the ones that will lead to the model working on simillar distributaions.\n",
        "# the opposite would be mezmorizing, which means overfitting the dataset in hand."
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0vnICIorl91"
      },
      "source": [
        "# the latter can be achieved using different strategies.\n",
        "# this depends on model complexity, dataset size, and evulation procedure and etc..\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2IfmbrmRsSB0"
      },
      "source": [
        "from d2l import mxnet as d2l\n",
        "from mxnet import gluon, np, npx\n",
        "from mxnet.gluon import nn\n",
        "import math\n",
        "npx.set_np()"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZToBT8O6uCtJ"
      },
      "source": [
        "$$y = 5 + 1.2x - 3.4\\frac{x^2}{2!} + 5.6 \\frac{x^3}{3!} + \\epsilon \\text{ where } \\epsilon \\sim \\mathcal{N}(0, 0.1^2).$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TbrFU3iNsSuS"
      },
      "source": [
        "max_degree = 20 # Maximum degree of the polynomial\n",
        "n_train, n_test = 100, 100 # Training and test dataset sizes\n",
        "true_w = np.zeros(max_degree) # Allocate lots of empty space\n",
        "true_w[0:4] = np.array([5, 1.2, -3.4, 5.6])\n",
        "features = np.random.normal(size=(n_train + n_test, 1))\n",
        "np.random.shuffle(features)\n",
        "poly_features = np.power(features, np.arange(max_degree).reshape(1, -1))\n",
        "for i in range(max_degree):\n",
        "  poly_features[:, i] /= math.gamma(i + 1) # `gamma(n)` = (n-1)!\n",
        "# Shape of `labels`: (`n_train` + `n_test`,)\n",
        "labels = np.dot(poly_features, true_w)\n",
        "labels += np.random.normal(scale=0.1, size=labels.shape)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0fFU-gDukUc",
        "outputId": "2df017cf-c356-4424-8321-ba6462582268",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254
        }
      },
      "source": [
        "features[:2], poly_features[:2, :], labels[:2]\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[-0.03716067],\n",
              "        [-1.1468065 ]]),\n",
              " array([[ 1.0000000e+00, -3.7160669e-02,  6.9045764e-04, -8.5526226e-06,\n",
              "          7.9455290e-08, -5.9052235e-10,  3.6573678e-12, -1.9415747e-14,\n",
              "          9.0187767e-17, -3.7238198e-19,  1.3837963e-21, -4.6747996e-24,\n",
              "          1.4476556e-26, -4.1381425e-29,  1.0984010e-31, -2.7211542e-34,\n",
              "          6.3199942e-37, -1.3815009e-39,  2.8516424e-42, -5.6051939e-45],\n",
              "        [ 1.0000000e+00, -1.1468065e+00,  6.5758252e-01, -2.5137332e-01,\n",
              "          7.2069131e-02, -1.6529869e-02,  3.1594271e-03, -5.1760738e-04,\n",
              "          7.4199430e-05, -9.4547095e-06,  1.0842723e-06, -1.1304095e-07,\n",
              "          1.0803007e-08, -9.5299690e-10,  7.8064499e-11, -5.9683248e-12,\n",
              "          4.2778208e-13, -2.8857840e-14,  1.8385754e-15, -1.1097317e-16]]),\n",
              " array([ 5.1432443 , -0.06415121]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4e1eCCIulLK",
        "outputId": "25c62086-1c5d-4f10-97f0-ebb03efdef30",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "print(features.shape)\n",
        "print(poly_features.shape)\n",
        "print(labels.shape)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(200, 1)\n",
            "(200, 20)\n",
            "(200,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxorZr5ZvBWk"
      },
      "source": [
        "def evaluate_loss(net, data_iter, loss): #@save\n",
        "  \"\"\"Evaluate the loss of a model on the given dataset.\"\"\"\n",
        "  metric = d2l.Accumulator(2) # Sum of losses, no. of examples\n",
        "  for X, y in data_iter:\n",
        "    l = loss(net(X), y)\n",
        "    metric.add(d2l.reduce_sum(l), l.size)\n",
        "  return metric[0] / metric[1]"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}